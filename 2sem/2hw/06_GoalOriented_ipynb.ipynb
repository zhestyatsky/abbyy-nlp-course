{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "\"06-GoalOriented.ipynb\"",
      "provenance": [],
      "collapsed_sections": [
        "APcntGZ0ReXl",
        "rAqVyqZ_SXxc",
        "NujuoWDU195f",
        "RyB8YIO6SH9T"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhestyatsky/abbyy-nlp-course/blob/main/2sem/2hw/06_GoalOriented_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCdNRLuY1lSP"
      },
      "source": [
        "# shorturl.at/iOY35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2POVHVpWnlb"
      },
      "source": [
        "Основано на: https://github.com/DanAnastasyev/DeepNLP-Course Week 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4WlMyJVRkzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df0d602-3db2-45f1-ef4e-740228b7b984"
      },
      "source": [
        "!git clone https://github.com/MiuLab/SlotGated-SLU.git\n",
        "!wget -qq https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/week08_multitask/conlleval.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SlotGated-SLU' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvJKy3mtVOpw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "DEVICE = torch.device('cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QR5dTAfVhLD"
      },
      "source": [
        "# Диалоговые системы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fox5ub_GKSLL"
      },
      "source": [
        "Диалоговые системы делятся на два типа - *goal-orientied* и *general conversation*.\n",
        "\n",
        "**General conversation** - это болталка, разговор на свободную тему:  \n",
        "<img src=\"https://i.ibb.co/bFwwGpc/alice.jpg\" width=\"200\"/>\n",
        "\n",
        "Сегодня будем говорить не про них, а про **goal-orientied** системы:\n",
        "\n",
        "<img src=\"https://hsto.org/webt/gj/3y/xl/gj3yxlqbr7ujuqr9r2akacxmkee.jpeg\" width=\"600\"/>\n",
        "\n",
        "*From [Как устроена Алиса](https://habr.com/company/yandex/blog/349372/)*\n",
        "\n",
        "Пользователь говорит что-то, это что-то распознается. По распознанному определяется - что, где и когда он хотел. Дальше диалоговый движок решает, действительно ли пользователь знает, чего хотел попросить. Происходит поход в источники - узнать информацию, которую (кажется) запросил пользователь. Исходя из всего этого генерируется некоторый ответ:\n",
        "\n",
        "<img src=\"https://i.ibb.co/8XcdpJ7/goal-orientied.png\" width=\"600\"/>\n",
        "\n",
        "*From [Как устроена Алиса](https://habr.com/company/yandex/blog/349372/)*\n",
        "\n",
        "Будем учить ту часть, которая посередине - классификатор и теггер. Всё остальное обычно - эвристики и захардкоженные ответы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIJt4hPLPYtO"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUZ8xjG_PT7C"
      },
      "source": [
        "Есть условно стандартный датасет - atis, который неприлично маленький, на самом деле.\n",
        "\n",
        "К нему можно взять еще датасет snips - он больше и разнообразнее.\n",
        "\n",
        "Оба датасета возьмем из репозитория статьи [Slot-Gated Modeling for Joint Slot Filling and Intent Prediction](http://aclweb.org/anthology/N18-2118).\n",
        "\n",
        "Начнем с atis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_FnOVOVgdX"
      },
      "source": [
        "import os \n",
        "\n",
        "def read_dataset(path):\n",
        "    with open(os.path.join(path, 'seq.in')) as f_words, \\\n",
        "            open(os.path.join(path, 'seq.out')) as f_tags, \\\n",
        "            open(os.path.join(path, 'label')) as f_intents:\n",
        "        \n",
        "        return [\n",
        "            (words.strip().split(), tags.strip().split(), intent.strip()) \n",
        "            for words, tags, intent in zip(f_words, f_tags, f_intents)\n",
        "        ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrAgjAFVWnh9"
      },
      "source": [
        "train_data = read_dataset('SlotGated-SLU/data/atis/train/')\n",
        "val_data = read_dataset('SlotGated-SLU/data/atis/valid/')\n",
        "test_data = read_dataset('SlotGated-SLU/data/atis/test/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3zvT5BsWv0p"
      },
      "source": [
        "intent_to_example = {example[2]: example for example in train_data}\n",
        "#for example in intent_to_example.values():\n",
        "#    print('Intent:\\t', example[2])\n",
        "#    print('Text:\\t', '\\t'.join(example[0]))\n",
        "#    print('Tags:\\t', '\\t'.join(example[1]))\n",
        "#    print()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRi9veisUDbA",
        "outputId": "97c9d3a3-6060-42db-d3a6-e368f3180d5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EoT_us7Y23P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86dbaeab-2f27-44a6-992d-fba88f17f9af"
      },
      "source": [
        "from torchtext.legacy.data import Field, LabelField, Example, Dataset, BucketIterator\n",
        "\n",
        "tokens_field = Field()\n",
        "tags_field = Field(unk_token=None)\n",
        "intent_field = LabelField()\n",
        "\n",
        "fields = [('tokens', tokens_field), ('tags', tags_field), ('intent', intent_field)]\n",
        "\n",
        "train_dataset = Dataset([Example.fromlist(example, fields) for example in train_data], fields)\n",
        "val_dataset = Dataset([Example.fromlist(example, fields) for example in val_data], fields)\n",
        "test_dataset = Dataset([Example.fromlist(example, fields) for example in test_data], fields)\n",
        "\n",
        "tokens_field.build_vocab(train_dataset)\n",
        "tags_field.build_vocab(train_dataset)\n",
        "intent_field.build_vocab(train_dataset)\n",
        "\n",
        "print('Vocab size =', len(tokens_field.vocab))\n",
        "print('Tags count =', len(tags_field.vocab))\n",
        "print('Intents count =', len(intent_field.vocab))\n",
        "\n",
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(train_dataset, val_dataset, test_dataset), batch_sizes=(32, 128, 128), \n",
        "    shuffle=True, sort=False\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size = 869\n",
            "Tags count = 121\n",
            "Intents count = 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPOfmYA6as2M"
      },
      "source": [
        "То же самое со snips"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bofp6semFwvG"
      },
      "source": [
        "snips_train_data = read_dataset('SlotGated-SLU/data/snips/train/')\n",
        "snips_val_data = read_dataset('SlotGated-SLU/data/snips/valid/')\n",
        "snips_test_data = read_dataset('SlotGated-SLU/data/snips/test/')\n",
        "snips_intent_to_example = {example[2]: example for example in snips_train_data}\n",
        "#for example in snips_intent_to_example.values():\n",
        "#    print('Intent:\\t', example[2])\n",
        "#    print('Text:\\t', '\\t'.join(example[0]))\n",
        "#    print('Tags:\\t', '\\t'.join(example[1]))\n",
        "#    print()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgoZ7sZSGQsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ae512f-c047-4416-d9ad-eab3e11b0923"
      },
      "source": [
        "from torchtext.legacy.data import Field, LabelField, Example, Dataset, BucketIterator\n",
        "\n",
        "snips_tokens_field = Field()\n",
        "snips_tags_field = Field(unk_token=None)\n",
        "snips_intent_field = LabelField()\n",
        "\n",
        "fields = [('tokens', snips_tokens_field), ('tags', snips_tags_field), ('intent', snips_intent_field)]\n",
        "\n",
        "snips_train_dataset = Dataset([Example.fromlist(example, fields) for example in snips_train_data], fields)\n",
        "snips_val_dataset = Dataset([Example.fromlist(example, fields) for example in snips_val_data], fields)\n",
        "snips_test_dataset = Dataset([Example.fromlist(example, fields) for example in snips_test_data], fields)\n",
        "\n",
        "snips_tokens_field.build_vocab(snips_train_dataset)\n",
        "snips_tags_field.build_vocab(snips_train_dataset)\n",
        "snips_intent_field.build_vocab(snips_train_dataset)\n",
        "\n",
        "print('Vocab size =', len(snips_tokens_field.vocab))\n",
        "print('Tags count =', len(snips_tags_field.vocab))\n",
        "print('Intents count =', len(snips_intent_field.vocab))\n",
        "\n",
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(snips_train_dataset, snips_val_dataset, snips_test_dataset), batch_sizes=(32, 128, 128), \n",
        "    shuffle=True, device = 'cpu', sort=False\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size = 11420\n",
            "Tags count = 73\n",
            "Intents count = 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8tY7_xQIpi"
      },
      "source": [
        "## Классификатор интентов\n",
        "\n",
        "Начнем с классификатора: к какому интенту относится данный запрос.\n",
        "\n",
        "Ничего умного - берём rnn'ку и учимся предсказывать метки-интенты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4pZR9IRckK-"
      },
      "source": [
        "class IntentClassifierModel(nn.Module):\n",
        "    def __init__(self, vocab_size, intents_count, emb_dim=64,\n",
        "                 lstm_hidden_dim=128, num_layers=1, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.lstm_layer = nn.LSTM(emb_dim, lstm_hidden_dim, batch_first=True,\n",
        "                                  bidirectional=True, num_layers=num_layers)\n",
        "        self.out_layer = nn.Linear(lstm_hidden_dim * 2, intents_count)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        projections = self.embeddings_layer.forward(inputs)\n",
        "        projections = projections.reshape(projections.size(0), projections.size(1), -1)\n",
        "        output, (final_hidden_state, _) = self.lstm_layer(projections)\n",
        "        hidden = self.dropout(torch.cat((final_hidden_state[0], final_hidden_state[1]), dim=1))\n",
        "        output = self.out_layer.forward(hidden)\n",
        "        return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfKFMfeg_RLV"
      },
      "source": [
        "class ModelTrainer():\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def on_epoch_begin(self, is_train, name, batches_count):\n",
        "        self.epoch_loss = 0\n",
        "        self.correct_count, self.total_count = 0, 0\n",
        "        self.is_train = is_train\n",
        "        self.name = name\n",
        "        self.batches_count = batches_count\n",
        "        self.model.train(is_train)\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "            self.name, self.epoch_loss / self.batches_count, self.correct_count / self.total_count\n",
        "        )\n",
        "        \n",
        "    def on_batch(self, batch):\n",
        "        logits = self.model(batch.tokens.transpose(0, 1))\n",
        "        loss = self.criterion(logits, batch.intent)\n",
        "        predicted_intent = torch.max(logits, axis=1)[1]\n",
        "        self.total_count += predicted_intent.size(0)\n",
        "        self.correct_count += torch.sum(predicted_intent == batch.intent).item()\n",
        "        if self.is_train:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "        self.epoch_loss += loss.item()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqCvQEByddtj"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "tqdm.get_lock().locks = []\n",
        "\n",
        "\n",
        "def do_epoch(trainer, data_iter, is_train, name=None):\n",
        "    trainer.on_epoch_begin(is_train, name, batches_count=len(data_iter))\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=trainer.batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):\n",
        "                batch_progress = trainer.on_batch(batch)\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description(batch_progress)\n",
        "                \n",
        "            epoch_progress = trainer.on_epoch_end()\n",
        "            progress_bar.set_description(epoch_progress)\n",
        "            progress_bar.refresh()\n",
        "\n",
        "            \n",
        "def fit(trainer, train_iter, epochs_count=1, val_iter=None):\n",
        "    best_val_loss = None\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        do_epoch(trainer, train_iter, is_train=True, name=name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            do_epoch(trainer, val_iter, is_train=False, name=name_prefix + '  Val:')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQBsP8SHhjqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95b9359-c6a8-486b-f1fc-17cc36aa38ac"
      },
      "source": [
        "model = IntentClassifierModel(vocab_size=len(snips_tokens_field.vocab), intents_count=len(snips_intent_field.vocab)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "trainer = ModelTrainer(model, criterion, optimizer)\n",
        "fit(trainer, train_iter = train_iter, epochs_count=30, val_iter=val_iter)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 30] Train: Loss = 0.39493, Accuracy = 88.70%: 100%|██████████| 409/409 [00:19<00:00, 20.81it/s]\n",
            "[1 / 30]   Val: Loss = 0.17411, Accuracy = 94.86%: 100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n",
            "[2 / 30] Train: Loss = 0.09135, Accuracy = 97.13%: 100%|██████████| 409/409 [00:17<00:00, 22.92it/s]\n",
            "[2 / 30]   Val: Loss = 0.12398, Accuracy = 96.57%: 100%|██████████| 6/6 [00:00<00:00, 23.69it/s]\n",
            "[3 / 30] Train: Loss = 0.05052, Accuracy = 98.43%: 100%|██████████| 409/409 [00:17<00:00, 22.77it/s]\n",
            "[3 / 30]   Val: Loss = 0.10799, Accuracy = 96.71%: 100%|██████████| 6/6 [00:00<00:00, 23.33it/s]\n",
            "[4 / 30] Train: Loss = 0.02993, Accuracy = 99.08%: 100%|██████████| 409/409 [00:18<00:00, 22.50it/s]\n",
            "[4 / 30]   Val: Loss = 0.16122, Accuracy = 96.14%: 100%|██████████| 6/6 [00:00<00:00, 23.59it/s]\n",
            "[5 / 30] Train: Loss = 0.02248, Accuracy = 99.38%: 100%|██████████| 409/409 [00:17<00:00, 22.82it/s]\n",
            "[5 / 30]   Val: Loss = 0.12737, Accuracy = 97.00%: 100%|██████████| 6/6 [00:00<00:00, 25.63it/s]\n",
            "[6 / 30] Train: Loss = 0.01237, Accuracy = 99.66%: 100%|██████████| 409/409 [00:18<00:00, 22.52it/s]\n",
            "[6 / 30]   Val: Loss = 0.09327, Accuracy = 97.00%: 100%|██████████| 6/6 [00:00<00:00, 23.13it/s]\n",
            "[7 / 30] Train: Loss = 0.00706, Accuracy = 99.80%: 100%|██████████| 409/409 [00:18<00:00, 22.53it/s]\n",
            "[7 / 30]   Val: Loss = 0.16840, Accuracy = 96.57%: 100%|██████████| 6/6 [00:00<00:00, 23.71it/s]\n",
            "[8 / 30] Train: Loss = 0.00596, Accuracy = 99.88%: 100%|██████████| 409/409 [00:18<00:00, 22.52it/s]\n",
            "[8 / 30]   Val: Loss = 0.19226, Accuracy = 96.86%: 100%|██████████| 6/6 [00:00<00:00, 24.07it/s]\n",
            "[9 / 30] Train: Loss = 0.00324, Accuracy = 99.93%: 100%|██████████| 409/409 [00:18<00:00, 22.39it/s]\n",
            "[9 / 30]   Val: Loss = 0.16909, Accuracy = 96.86%: 100%|██████████| 6/6 [00:00<00:00, 23.97it/s]\n",
            "[10 / 30] Train: Loss = 0.00322, Accuracy = 99.94%: 100%|██████████| 409/409 [00:18<00:00, 22.41it/s]\n",
            "[10 / 30]   Val: Loss = 0.20063, Accuracy = 96.86%: 100%|██████████| 6/6 [00:00<00:00, 24.84it/s]\n",
            "[11 / 30] Train: Loss = 0.00067, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.67it/s]\n",
            "[11 / 30]   Val: Loss = 0.20951, Accuracy = 97.14%: 100%|██████████| 6/6 [00:00<00:00, 23.29it/s]\n",
            "[12 / 30] Train: Loss = 0.00037, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.72it/s]\n",
            "[12 / 30]   Val: Loss = 0.22124, Accuracy = 96.86%: 100%|██████████| 6/6 [00:00<00:00, 23.70it/s]\n",
            "[13 / 30] Train: Loss = 0.00235, Accuracy = 99.94%: 100%|██████████| 409/409 [00:18<00:00, 22.25it/s]\n",
            "[13 / 30]   Val: Loss = 0.15050, Accuracy = 96.86%: 100%|██████████| 6/6 [00:00<00:00, 23.35it/s]\n",
            "[14 / 30] Train: Loss = 0.01120, Accuracy = 99.64%: 100%|██████████| 409/409 [00:18<00:00, 22.03it/s]\n",
            "[14 / 30]   Val: Loss = 0.11969, Accuracy = 96.86%: 100%|██████████| 6/6 [00:00<00:00, 22.95it/s]\n",
            "[15 / 30] Train: Loss = 0.00430, Accuracy = 99.89%: 100%|██████████| 409/409 [00:18<00:00, 22.13it/s]\n",
            "[15 / 30]   Val: Loss = 0.13295, Accuracy = 97.14%: 100%|██████████| 6/6 [00:00<00:00, 23.46it/s]\n",
            "[16 / 30] Train: Loss = 0.00086, Accuracy = 99.98%: 100%|██████████| 409/409 [00:18<00:00, 22.33it/s]\n",
            "[16 / 30]   Val: Loss = 0.12584, Accuracy = 97.00%: 100%|██████████| 6/6 [00:00<00:00, 22.62it/s]\n",
            "[17 / 30] Train: Loss = 0.00029, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.24it/s]\n",
            "[17 / 30]   Val: Loss = 0.13030, Accuracy = 97.14%: 100%|██████████| 6/6 [00:00<00:00, 23.51it/s]\n",
            "[18 / 30] Train: Loss = 0.00013, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.26it/s]\n",
            "[18 / 30]   Val: Loss = 0.13053, Accuracy = 97.29%: 100%|██████████| 6/6 [00:00<00:00, 24.48it/s]\n",
            "[19 / 30] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.43it/s]\n",
            "[19 / 30]   Val: Loss = 0.16021, Accuracy = 97.29%: 100%|██████████| 6/6 [00:00<00:00, 23.37it/s]\n",
            "[20 / 30] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.42it/s]\n",
            "[20 / 30]   Val: Loss = 0.16474, Accuracy = 97.29%: 100%|██████████| 6/6 [00:00<00:00, 22.92it/s]\n",
            "[21 / 30] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.44it/s]\n",
            "[21 / 30]   Val: Loss = 0.16928, Accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 22.13it/s]\n",
            "[22 / 30] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.31it/s]\n",
            "[22 / 30]   Val: Loss = 0.16071, Accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 23.08it/s]\n",
            "[23 / 30] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.32it/s]\n",
            "[23 / 30]   Val: Loss = 0.15301, Accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 23.22it/s]\n",
            "[24 / 30] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.54it/s]\n",
            "[24 / 30]   Val: Loss = 0.16688, Accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 24.14it/s]\n",
            "[25 / 30] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.53it/s]\n",
            "[25 / 30]   Val: Loss = 0.19145, Accuracy = 97.57%: 100%|██████████| 6/6 [00:00<00:00, 24.00it/s]\n",
            "[26 / 30] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.26it/s]\n",
            "[26 / 30]   Val: Loss = 0.17585, Accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 24.18it/s]\n",
            "[27 / 30] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.14it/s]\n",
            "[27 / 30]   Val: Loss = 0.16523, Accuracy = 97.57%: 100%|██████████| 6/6 [00:00<00:00, 23.82it/s]\n",
            "[28 / 30] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.20it/s]\n",
            "[28 / 30]   Val: Loss = 0.17995, Accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 21.75it/s]\n",
            "[29 / 30] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 409/409 [00:19<00:00, 21.35it/s]\n",
            "[29 / 30]   Val: Loss = 0.18097, Accuracy = 97.57%: 100%|██████████| 6/6 [00:00<00:00, 24.19it/s]\n",
            "[30 / 30] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 409/409 [00:18<00:00, 22.24it/s]\n",
            "[30 / 30]   Val: Loss = 0.18483, Accuracy = 97.57%: 100%|██████████| 6/6 [00:00<00:00, 23.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxHshnyZjMuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1b574c-13ab-499b-9aa9-0599f6a4a719"
      },
      "source": [
        "do_epoch(trainer, test_iter, is_train=False, name='Test:')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: Loss = 0.24093, Accuracy = 95.71%: 100%|██████████| 6/6 [00:00<00:00, 22.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zsAJJCEQ8Ti"
      },
      "source": [
        "## Теггер\n",
        "\n",
        "![](https://commons.bmstu.wiki/images/0/00/NER1.png)  \n",
        "*From [NER](https://ru.bmstu.wiki/NER_(Named-Entity_Recognition)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUZ1Wmw1a-Qo"
      },
      "source": [
        "#### **Задание 1.1**\n",
        "Напишите простой теггер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwphVxmdkChy"
      },
      "source": [
        "class TokenTaggerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, tags_count, emb_dim=64,\n",
        "                 lstm_hidden_dim=128, num_layers=1, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.lstm_layer = nn.LSTM(emb_dim, lstm_hidden_dim, batch_first=True,\n",
        "                                  bidirectional=True, num_layers=num_layers)\n",
        "        self.out_layer = nn.Linear(2*lstm_hidden_dim, tags_count)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        projections = self.embeddings_layer.forward(inputs)\n",
        "        last_lstm_layer_output, _ = self.lstm_layer(projections)\n",
        "        output = self.dropout(last_lstm_layer_output)\n",
        "        output = self.out_layer.forward(output)\n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mzyxM0502wy"
      },
      "source": [
        "#### **Задание 1.2**\n",
        "Обновите `ModelTrainer`: считать нужно всё те же лосс и accuracy, только теперь немного по-другому."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMRwby_NnyvJ"
      },
      "source": [
        "class TagModelTrainer():\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def on_epoch_begin(self, is_train, name, batches_count):\n",
        "        self.epoch_loss = 0\n",
        "        self.correct_count, self.total_count = 0, 0\n",
        "        self.is_train = is_train\n",
        "        self.name = name\n",
        "        self.batches_count = batches_count\n",
        "        self.model.train(is_train)\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "            self.name, self.epoch_loss / self.batches_count, self.correct_count / self.total_count\n",
        "        )\n",
        "        \n",
        "    def on_batch(self, batch):\n",
        "      # batch_size x n_tokens\n",
        "      tokens = batch.tokens.t()\n",
        "      tag_labels = batch.tags.t()\n",
        "\n",
        "      # batch_size x n_tokens x n_tags\n",
        "      outputs = self.model(tokens)\n",
        "\n",
        "      # batch_size x n_tags x n_tokens\n",
        "      logits = outputs.transpose(1, 2)\n",
        "      loss = self.criterion(logits, tag_labels)\n",
        "\n",
        "      # batch_size x n_tokens\n",
        "      tag_prediction_indices = outputs.max(axis=2)[1]\n",
        "\n",
        "      self.correct_count += torch.sum(tag_labels == tag_prediction_indices).item() - torch.sum(tag_labels == 0).item()\n",
        "      self.total_count += torch.sum(tag_labels > 0).item()\n",
        "\n",
        "      if self.is_train:\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()\n",
        "      self.epoch_loss += loss.item()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QXaapt3nuF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b43c529-43e6-4aac-b71a-88beba000e3a"
      },
      "source": [
        "model = TokenTaggerModel(vocab_size=len(snips_tokens_field.vocab), tags_count=len(snips_tags_field.vocab)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "trainer = TagModelTrainer(model, criterion, optimizer)\n",
        "fit(trainer, train_iter, epochs_count=10, val_iter=val_iter)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 10] Train: Loss = 0.87282, Accuracy = 62.10%: 100%|██████████| 409/409 [00:23<00:00, 17.41it/s]\n",
            "[1 / 10]   Val: Loss = 0.35810, Accuracy = 79.93%: 100%|██████████| 6/6 [00:00<00:00, 17.54it/s]\n",
            "[2 / 10] Train: Loss = 0.29251, Accuracy = 84.43%: 100%|██████████| 409/409 [00:22<00:00, 17.93it/s]\n",
            "[2 / 10]   Val: Loss = 0.21084, Accuracy = 87.22%: 100%|██████████| 6/6 [00:00<00:00, 17.84it/s]\n",
            "[3 / 10] Train: Loss = 0.18526, Accuracy = 89.75%: 100%|██████████| 409/409 [00:22<00:00, 17.83it/s]\n",
            "[3 / 10]   Val: Loss = 0.16562, Accuracy = 89.80%: 100%|██████████| 6/6 [00:00<00:00, 18.25it/s]\n",
            "[4 / 10] Train: Loss = 0.13427, Accuracy = 92.47%: 100%|██████████| 409/409 [00:22<00:00, 18.53it/s]\n",
            "[4 / 10]   Val: Loss = 0.13980, Accuracy = 91.71%: 100%|██████████| 6/6 [00:00<00:00, 18.84it/s]\n",
            "[5 / 10] Train: Loss = 0.10253, Accuracy = 94.21%: 100%|██████████| 409/409 [00:23<00:00, 17.42it/s]\n",
            "[5 / 10]   Val: Loss = 0.11380, Accuracy = 92.67%: 100%|██████████| 6/6 [00:00<00:00, 19.78it/s]\n",
            "[6 / 10] Train: Loss = 0.08054, Accuracy = 95.56%: 100%|██████████| 409/409 [00:21<00:00, 18.73it/s]\n",
            "[6 / 10]   Val: Loss = 0.10165, Accuracy = 93.41%: 100%|██████████| 6/6 [00:00<00:00, 19.91it/s]\n",
            "[7 / 10] Train: Loss = 0.06388, Accuracy = 96.54%: 100%|██████████| 409/409 [00:22<00:00, 18.41it/s]\n",
            "[7 / 10]   Val: Loss = 0.10574, Accuracy = 93.78%: 100%|██████████| 6/6 [00:00<00:00, 20.77it/s]\n",
            "[8 / 10] Train: Loss = 0.05110, Accuracy = 97.22%: 100%|██████████| 409/409 [00:22<00:00, 17.85it/s]\n",
            "[8 / 10]   Val: Loss = 0.10371, Accuracy = 93.95%: 100%|██████████| 6/6 [00:00<00:00, 20.08it/s]\n",
            "[9 / 10] Train: Loss = 0.04080, Accuracy = 97.90%: 100%|██████████| 409/409 [00:23<00:00, 17.53it/s]\n",
            "[9 / 10]   Val: Loss = 0.09595, Accuracy = 94.19%: 100%|██████████| 6/6 [00:00<00:00, 19.43it/s]\n",
            "[10 / 10] Train: Loss = 0.03277, Accuracy = 98.33%: 100%|██████████| 409/409 [00:22<00:00, 18.11it/s]\n",
            "[10 / 10]   Val: Loss = 0.09253, Accuracy = 94.28%: 100%|██████████| 6/6 [00:00<00:00, 18.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6spi0nfWA6uK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afed37a7-dde2-4076-d88b-646c06bec969"
      },
      "source": [
        "do_epoch(trainer, test_iter, is_train=False, name='Test:')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: Loss = 0.10671, Accuracy = 93.36%: 100%|██████████| 6/6 [00:00<00:00, 17.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conlleval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTbThgneHmD8",
        "outputId": "6665014f-f803-46a3-91b4-ab1814fe40ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conlleval in /usr/local/lib/python3.7/dist-packages (0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKeXWjs7pE35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074e0f45-d5c6-4c79-a46b-4832175ff397"
      },
      "source": [
        "from conlleval import evaluate\n",
        "\n",
        "def eval_tagger(model, test_iter):\n",
        "    true_seqs, pred_seqs = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            pred = model.forward(batch.tokens.transpose(0, 1)).transpose(1, 2).max(dim=1)[1].cpu().tolist()\n",
        "            pred_seqs.extend([\" \".join([tags_field.vocab.itos[elem] for elem in l if elem != 0]) for l in pred])\n",
        "            true_seqs.extend([\" \".join([tags_field.vocab.itos[elem] for elem in l if elem != 0]) for l in batch.tags.transpose(0, 1).cpu().tolist()])\n",
        "\n",
        "    print('Precision = {:.2f}%, Recall = {:.2f}%, F1 = {:.2f}%'.format(*evaluate(true_seqs, pred_seqs, verbose=False)))\n",
        "\n",
        "eval_tagger(model, test_iter)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision = 89.36%, Recall = 89.22%, F1 = 89.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APcntGZ0ReXl"
      },
      "source": [
        "## Multi-task learning\n",
        "\n",
        "Реализуем модель, которая умеет сразу и предсказывать теги и интенты. Идея в том, что в этом всем есть общая информация, которая должна помочь как одной, так и другой задаче: зная интент, можно понять, какие слоты вообще могут быть, а зная слоты, можно угадать и интент."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkWuh_QMbeOM"
      },
      "source": [
        "#### **Задание 2.1**\n",
        "Реализуйте объединенную модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goLcDk-Tu0uM"
      },
      "source": [
        "class SharedModel(nn.Module):\n",
        "    def __init__(self, vocab_size, intents_count, tags_count, emb_dim=64,\n",
        "                 lstm_hidden_dim=128, num_layers=1, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.lstm_layer = nn.LSTM(emb_dim, lstm_hidden_dim, batch_first=True,\n",
        "                                  bidirectional=True, num_layers=num_layers)\n",
        "        self.out_layer_intent = nn.Linear(2*lstm_hidden_dim, intents_count)\n",
        "        self.out_layer_tags = nn.Linear(2*lstm_hidden_dim, tags_count)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        projection = self.embedding_layer.forward(inputs)\n",
        "        output, (hidden, _) = self.lstm_layer(projection)\n",
        "        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
        "        \n",
        "        output = self.dropout(output)\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        tags_output = self.out_layer_tags.forward(output)\n",
        "        intent_output = self.out_layer_intent.forward(hidden)\n",
        "        \n",
        "        return tags_output, intent_output"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2A9hHybrWZ"
      },
      "source": [
        "#### **Задание 2.2**\n",
        "Допишите SharedModelTrainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5semraSfv56f"
      },
      "source": [
        "class SharedModelTrainer():\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def on_epoch_begin(self, is_train, name, batches_count):\n",
        "        self.epoch_loss = 0\n",
        "        self.tag_correct_count, self.tag_total_count = 0, 0\n",
        "        self.intent_correct_count, self.intent_total_count = 0, 0\n",
        "        self.is_train = is_train\n",
        "        self.name = name\n",
        "        self.batches_count = batches_count\n",
        "        self.model.train(is_train)\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        return '{:>5s} Loss = {:.5f}, Tags accuracy = {:.2%}, Intents accuracy = {:.2%}'.format(\n",
        "            self.name, self.epoch_loss / self.batches_count, self.tag_correct_count / self.tag_total_count, \n",
        "            self.intent_correct_count / self.intent_total_count\n",
        "        )\n",
        "        \n",
        "    def on_batch(self, batch):\n",
        "      tokens = batch.tokens.t()\n",
        "      tag_labels = batch.tags.t()\n",
        "      intent_labels = batch.intent\n",
        "\n",
        "      tags_outputs, intent_outputs = self.model(tokens)\n",
        "      tags_logits = tags_outputs.transpose(1, 2)\n",
        "\n",
        "      tags_loss = self.criterion(tags_logits, tag_labels)\n",
        "      intent_loss = self.criterion(intent_outputs, intent_labels)\n",
        "      loss = tags_loss + intent_loss\n",
        "\n",
        "      tag_prediction_indices = tags_outputs.max(axis=2)[1]\n",
        "      intent_prediction_indices = intent_outputs.max(axis=1)[1]\n",
        "\n",
        "      self.tag_correct_count += torch.sum(tag_labels == tag_prediction_indices).item() - torch.sum(tag_labels == 0).item()\n",
        "      self.tag_total_count += torch.sum(tag_labels > 0).item()\n",
        "      \n",
        "      self.intent_correct_count += torch.sum(intent_labels == intent_prediction_indices).item()\n",
        "      self.intent_total_count += intent_labels.size(0)\n",
        "\n",
        "      if self.is_train:\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()\n",
        "\n",
        "      self.epoch_loss += loss.item()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BP4b-4zxU0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359a4d60-55c0-49c9-f723-618d9384005e"
      },
      "source": [
        "model = SharedModel(vocab_size=len(snips_tokens_field.vocab), intents_count=len(snips_intent_field.vocab),\n",
        "                    tags_count=len(snips_tags_field.vocab)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "trainer = SharedModelTrainer(model, criterion, optimizer)\n",
        "fit(trainer, train_iter, epochs_count=10, val_iter=val_iter)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 10] Train: Loss = 1.36672, Tags accuracy = 60.62%, Intents accuracy = 86.53%: 100%|██████████| 409/409 [00:25<00:00, 16.07it/s]\n",
            "[1 / 10]   Val: Loss = 0.49822, Tags accuracy = 78.13%, Intents accuracy = 97.00%: 100%|██████████| 6/6 [00:00<00:00, 19.63it/s]\n",
            "[2 / 10] Train: Loss = 0.42029, Tags accuracy = 82.26%, Intents accuracy = 97.27%: 100%|██████████| 409/409 [00:22<00:00, 18.00it/s]\n",
            "[2 / 10]   Val: Loss = 0.34908, Tags accuracy = 85.32%, Intents accuracy = 96.71%: 100%|██████████| 6/6 [00:00<00:00, 20.39it/s]\n",
            "[3 / 10] Train: Loss = 0.27444, Tags accuracy = 87.59%, Intents accuracy = 98.48%: 100%|██████████| 409/409 [00:22<00:00, 18.28it/s]\n",
            "[3 / 10]   Val: Loss = 0.25962, Tags accuracy = 88.41%, Intents accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 17.91it/s]\n",
            "[4 / 10] Train: Loss = 0.20393, Tags accuracy = 90.48%, Intents accuracy = 98.99%: 100%|██████████| 409/409 [00:23<00:00, 17.05it/s]\n",
            "[4 / 10]   Val: Loss = 0.21875, Tags accuracy = 89.41%, Intents accuracy = 98.29%: 100%|██████████| 6/6 [00:00<00:00, 20.98it/s]\n",
            "[5 / 10] Train: Loss = 0.15791, Tags accuracy = 92.30%, Intents accuracy = 99.33%: 100%|██████████| 409/409 [00:21<00:00, 18.67it/s]\n",
            "[5 / 10]   Val: Loss = 0.20604, Tags accuracy = 90.40%, Intents accuracy = 98.14%: 100%|██████████| 6/6 [00:00<00:00, 20.49it/s]\n",
            "[6 / 10] Train: Loss = 0.12619, Tags accuracy = 93.75%, Intents accuracy = 99.49%: 100%|██████████| 409/409 [00:23<00:00, 17.75it/s]\n",
            "[6 / 10]   Val: Loss = 0.22637, Tags accuracy = 91.84%, Intents accuracy = 97.86%: 100%|██████████| 6/6 [00:00<00:00, 19.38it/s]\n",
            "[7 / 10] Train: Loss = 0.10354, Tags accuracy = 94.78%, Intents accuracy = 99.62%: 100%|██████████| 409/409 [00:23<00:00, 17.75it/s]\n",
            "[7 / 10]   Val: Loss = 0.19987, Tags accuracy = 91.68%, Intents accuracy = 97.86%: 100%|██████████| 6/6 [00:00<00:00, 19.76it/s]\n",
            "[8 / 10] Train: Loss = 0.08188, Tags accuracy = 95.72%, Intents accuracy = 99.83%: 100%|██████████| 409/409 [00:22<00:00, 18.40it/s]\n",
            "[8 / 10]   Val: Loss = 0.19977, Tags accuracy = 92.31%, Intents accuracy = 97.43%: 100%|██████████| 6/6 [00:00<00:00, 20.01it/s]\n",
            "[9 / 10] Train: Loss = 0.06200, Tags accuracy = 96.69%, Intents accuracy = 99.98%: 100%|██████████| 409/409 [00:22<00:00, 18.37it/s]\n",
            "[9 / 10]   Val: Loss = 0.20576, Tags accuracy = 92.40%, Intents accuracy = 97.57%: 100%|██████████| 6/6 [00:00<00:00, 17.57it/s]\n",
            "[10 / 10] Train: Loss = 0.04985, Tags accuracy = 97.34%, Intents accuracy = 100.00%: 100%|██████████| 409/409 [00:23<00:00, 17.64it/s]\n",
            "[10 / 10]   Val: Loss = 0.20748, Tags accuracy = 93.15%, Intents accuracy = 97.86%: 100%|██████████| 6/6 [00:00<00:00, 18.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlTbVSOezMD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4acbbb8-53a6-4986-c3d1-17de239a97d8"
      },
      "source": [
        "do_epoch(trainer, test_iter, is_train=False, name='Test:')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: Loss = 0.23148, Tags accuracy = 92.97%, Intents accuracy = 97.00%: 100%|██████████| 6/6 [00:00<00:00, 15.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-6XO9lsyhJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e0f172-a8b7-4dea-a07d-b673c2e3e6ff"
      },
      "source": [
        "from conlleval import evaluate\n",
        "\n",
        "def eval_tagger(model, test_iter):\n",
        "    true_seqs, pred_seqs = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            pred = model.forward(batch.tokens.transpose(0, 1))[0].transpose(1, 2).max(dim=1)[1].cpu().tolist()\n",
        "            true = batch.tags.transpose(0, 1).cpu().tolist()\n",
        "            pred_seqs.extend([\" \".join([tags_field.vocab.itos[elem] for elem in l if elem != 0]) for l in pred])\n",
        "            true_seqs.extend([\" \".join([tags_field.vocab.itos[elem] for elem in l if elem != 0]) for l in true])\n",
        "\n",
        "    print('Precision = {:.2f}%, Recall = {:.2f}%, F1 = {:.2f}%'.format(*evaluate(true_seqs, pred_seqs, verbose=False)))\n",
        "\n",
        "eval_tagger(model, test_iter)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision = 88.70%, Recall = 88.47%, F1 = 88.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAqVyqZ_SXxc"
      },
      "source": [
        " ## Асинхронное обучение\n",
        "\n",
        "Идея описана в статье [A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling](http://aclweb.org/anthology/N18-2050).\n",
        "\n",
        "<img src=\"https://i.ibb.co/qrgVSqF/2018-11-27-2-11-17.png\" width=\"600\"/>\n",
        "\n",
        "Основное отличие от того, что уже реализовали в том, в каком порядке все оптимизируется. Вместо объединенного обучения всех слоев, сети для теггера и для классификатора обучаются отдельно.\n",
        "\n",
        "На каждом шаге обучения генерируются последовательности скрытых состояний $h^1$ и $h^2$ - для классификатора и для теггера.\n",
        "\n",
        "Дальше сначала считаются потери от предсказания интента и делается шаг оптимизатора, а затем потери от предсказания теггов - и опять шаг оптимизатора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fadD_b4Lb5PW"
      },
      "source": [
        "#### **Задание 3.1**\n",
        "Реализуйте асинхронное обучение совместной модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLlVOYfG-dRY"
      },
      "source": [
        "class AsyncSharedModel(nn.Module):\n",
        "    def __init__(self, vocab_size, intents_count, tags_count, emb_dim=65,\n",
        "                 lstm_hidden_dim=128, num_layers=1, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # YOUR CODE HERE\n",
        "        return tag_output, intent_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEYVn6IXBRiR"
      },
      "source": [
        "class AsyncSharedModelTrainer():\n",
        "    def __init__(self, model, criterion, tags_optimizer, intent_optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.tags_optimizer = tags_optimizer\n",
        "        self.intent_optimizer = intent_optimizer\n",
        "        \n",
        "    def on_epoch_begin(self, is_train, name, batches_count):\n",
        "        self.epoch_loss = 0\n",
        "        self.tag_correct_count, self.tag_total_count = 0, 0\n",
        "        self.intent_correct_count, self.intent_total_count = 0, 0\n",
        "        self.is_train = is_train\n",
        "        self.name = name\n",
        "        self.batches_count = batches_count\n",
        "        self.model.train(is_train)\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        return '{:>5s} Loss = {:.5f}, Tags accuracy = {:.2%}, Intents accuracy = {:.2%}'.format(\n",
        "            self.name, self.epoch_loss / self.batches_count, self.tag_correct_count / self.tag_total_count, \n",
        "            self.intent_correct_count / self.intent_total_count\n",
        "        )\n",
        "        \n",
        "    def on_batch(self, batch):\n",
        "        # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMNscGmZ4APl"
      },
      "source": [
        "Затем их нужно передать в отдельные оптимизаторы и учить отдельно.\n",
        "\n",
        "*Еще, может быть, пригодится retain_graph параметр метода backward()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmMt811LBUXb"
      },
      "source": [
        "model = AsyncSharedModel(vocab_size=len(tokens_field.vocab), intents_count=len(intent_field.vocab),\n",
        "                         tags_count=len(tags_field.vocab)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "tags_parameters = [param for name, param in model.named_parameters() if not 'intent' in name]\n",
        "intent_parameters = [param for name, param in model.named_parameters() if not 'tags' in name]\n",
        "tags_optimizer = optim.Adam(tags_parameters)\n",
        "intent_optimizer = optim.Adam(intent_parameters)\n",
        "trainer = AsyncSharedModelTrainer(model, criterion, tags_optimizer, intent_optimizer)\n",
        "fit(trainer, train_iter, epochs_count=30, val_iter=val_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbroXzacJJlv"
      },
      "source": [
        "do_epoch(trainer, test_iter, is_train=False, name='Test:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnMw5L4iG5Io"
      },
      "source": [
        "from conlleval import evaluate\n",
        "\n",
        "def eval_tagger(model, test_iter):\n",
        "    true_seqs, pred_seqs = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            pred = model.forward(batch.tokens.transpose(0, 1))[0].transpose(1, 2).max(dim=1)[1].cpu().tolist()\n",
        "            pred_seqs.extend([\" \".join([tags_field.vocab.itos[elem] for elem in l if elem != 0]) for l in pred])\n",
        "            true_seqs.extend([\" \".join([tags_field.vocab.itos[elem] for elem in l if elem != 0]) for l in batch.tags.transpose(0, 1).cpu().tolist()])\n",
        "\n",
        "    print('Precision = {:.2f}%, Recall = {:.2f}%, F1 = {:.2f}%'.format(*evaluate(true_seqs, pred_seqs, verbose=False)))\n",
        "\n",
        "eval_tagger(model, test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE-5oyMUU40L"
      },
      "source": [
        "#### **Задание 3.2**\n",
        "Посмотрите на параметры в статье и попробуйте добиться похожего качества.\n",
        "\n",
        "#### **Задание 4**\n",
        "Посмотрите результаты на SNIPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NujuoWDU195f"
      },
      "source": [
        "## Async Multi-task Learning for POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9Q0ip3p2a5v"
      },
      "source": [
        "Ещё одна статья: [Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings](https://arxiv.org/pdf/1805.08237.pdf)\n",
        "\n",
        "Архитектура там такая:\n",
        "\n",
        "<img src=\"https://i.ibb.co/0nSX6CC/2018-11-27-9-26-15.png\" width=\"400\"/>\n",
        "\n",
        "Multi-task задача - обучение отдельных классификаторов более низкого уровня (над символами и словами) для предсказания тегов отдельными оптимизаторами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyB8YIO6SH9T"
      },
      "source": [
        "## DeepPavlov go_bot\n",
        "\n",
        "http://docs.deeppavlov.ai/en/master/features/skills/go_bot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBonKW_7SSOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09bdbe7b-d5c3-4b2b-c888-0eab01dfc431"
      },
      "source": [
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install gobot_dstc2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deeppavlov\n",
            "  Downloading deeppavlov-0.17.2-py3-none-any.whl (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "  Downloading fastapi-0.47.1-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting pyopenssl==19.1.0\n",
            "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
            "\u001b[K     |████████████████████████████████| 859 kB 41.2 MB/s \n",
            "\u001b[?25hCollecting Cython==0.29.14\n",
            "  Downloading Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.62.0\n",
            "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "  Downloading pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting pytz==2019.1\n",
            "  Downloading pytz-2019.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 45.2 MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.7\n",
            "  Downloading uvicorn-0.11.7-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "  Downloading ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654 kB)\n",
            "\u001b[K     |████████████████████████████████| 654 kB 38.0 MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
            "Collecting numpy==1.18.0\n",
            "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 517 kB/s \n",
            "\u001b[?25hCollecting overrides==2.7.0\n",
            "  Downloading overrides-2.7.0.tar.gz (4.5 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 47.1 MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14.0\n",
            "  Downloading uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting pydantic==1.3\n",
            "  Downloading pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Collecting pandas==0.25.3\n",
            "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 44.7 MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting pymorphy2-dicts-ru\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "  Downloading aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 17 kB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "  Downloading pyTelegramBotAPI-3.6.7.tar.gz (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting scikit-learn==0.21.2\n",
            "  Downloading scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 35.1 MB/s \n",
            "\u001b[?25hCollecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 39.1 MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading aiormq-3.3.1-py3-none-any.whl (28 kB)\n",
            "Collecting yarl\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 56.1 MB/s \n",
            "\u001b[?25hCollecting starlette<=0.12.9,>=0.12.9\n",
            "  Downloading starlette-0.12.9.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.8\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov) (1.1.0)\n",
            "Collecting httptools==0.1.*\n",
            "  Downloading httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219 kB)\n",
            "\u001b[K     |████████████████████████████████| 219 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading pamqp-2.3.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.21)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.10.0.2)\n",
            "Collecting multidict>=4.0\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nltk, overrides, prometheus-client, pytelegrambotapi, sacremoses, starlette\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449922 sha256=f7b10cc7d0110e09755dbefa0b38c01d642b63ba260cf8e353494b573c36a8b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-py3-none-any.whl size=5603 sha256=641bf4511ed1f8d87966a26abcfed2ad74d016f7898bd54f4719e9af49dab4e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/87/45/bfdacf6c3b8233b6e8d519edcbd1cf297ad5ff5f0bf84bb9c1\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-py3-none-any.whl size=41405 sha256=ac0ddbb5ad0d1241b13f97aeb8ddd8fe36b419d0d573210175b5befdcbcfb680\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/0c/26/59ba285bf65dc79d195e9b25e2ddde4c61070422729b0cd914\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-py3-none-any.whl size=47176 sha256=9ba7aee1df672761d1739948d72e5446fd140d7d3e3e325f296b7ee466d6143b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/7c/54/8eddf2369ef1b9190e2ee6dc2b40df54b6c65529a38790fdd4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=504609c9e398d959fc2deae577924cfb45f7753680cc769f58adf6a32ecc3c77\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ff/0e/e00ff1e22100702ac8b24e709551ae0fb29db9ffc843510a64\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-py3-none-any.whl size=57252 sha256=3a9c9730701d12ff9ac561d7ad5ffb83082207002cc19f1dd5b2bd62af95c61b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/78/be/f57ed5aed7cd222abdb24e3186b5c9f1074184fcc0a295102b\n",
            "Successfully built nltk overrides prometheus-client pytelegrambotapi sacremoses starlette\n",
            "Installing collected packages: multidict, idna, yarl, pamqp, numpy, websockets, uvloop, tqdm, starlette, requests, pytz, pymorphy2-dicts, pydantic, httptools, h11, dawg-python, cryptography, aiormq, uvicorn, scikit-learn, sacremoses, rusenttokenize, ruamel.yaml, pytelegrambotapi, pyopenssl, pymorphy2-dicts-ru, pymorphy2, prometheus-client, pandas, overrides, nltk, h5py, filelock, fastapi, Cython, aio-pika, deeppavlov\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.13.1\n",
            "    Uninstalling prometheus-client-0.13.1:\n",
            "      Successfully uninstalled prometheus-client-0.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.6.0\n",
            "    Uninstalling filelock-3.6.0:\n",
            "      Successfully uninstalled filelock-3.6.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.28\n",
            "    Uninstalling Cython-0.29.28:\n",
            "      Successfully uninstalled Cython-0.29.28\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "xarray 0.18.2 requires pandas>=1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.0 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
            "jaxlib 0.3.0+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.0 which is incompatible.\n",
            "jax 0.3.1 requires numpy>=1.19, but you have numpy 1.18.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 0.25.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-36.0.1 dawg-python-0.7.2 deeppavlov-0.17.2 fastapi-0.47.1 filelock-3.0.12 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 idna-2.8 multidict-6.0.2 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 tqdm-4.62.0 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "pandas",
                  "pytz",
                  "requests",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-01 10:55:34.188 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'gobot_dstc2' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/go_bot/gobot_dstc2.json'\n",
            "Collecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.44.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.13.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.7.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=3608e620928d75f42c6296dbebfc180bbee6da7860f09b164a97f96b1e1e1f64\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1019, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1653, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1611, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/deeppavlov/__main__.py\", line 4, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/deeppavlov/deep.py\", line 130, in main\n",
            "    install_from_config(pipeline_config_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/deeppavlov/utils/pip_wrapper/pip_wrapper.py\", line 71, in install_from_config\n",
            "    install(r)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/deeppavlov/utils/pip_wrapper/pip_wrapper.py\", line 38, in install\n",
            "    env=os.environ.copy())\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 358, in check_call\n",
            "    retcode = call(*popenargs, **kwargs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 341, in call\n",
            "    return p.wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1032, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1636, in _wait\n",
            "    (pid, sts) = self._try_wait(os.WNOHANG)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBh7aB9WSQ3m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6a9f4e5-7741-4dc8-f5da-651cd1071a07"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "from deeppavlov import build_model, configs\n",
        "\n",
        "bot1 = build_model(configs.go_bot.gobot_dstc2, download=True)\n",
        "\n",
        "bot1(['hi, i want restaurant in the cheap pricerange'])\n",
        "bot1(['bye'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-01 10:56:07.732 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt to /root/.deeppavlov/downloads/embeddings/glove.6B.100d.txt\n",
            "347MB [00:07, 46.8MB/s]\n",
            "2022-03-01 10:56:16.549 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/dstc_slot_vals.tar.gz to /root/.deeppavlov/downloads/dstc_slot_vals.tar.gz\n",
            "100%|██████████| 1.62k/1.62k [00:00<00:00, 173kB/s]\n",
            "2022-03-01 10:56:17.242 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/downloads/dstc_slot_vals.tar.gz archive into /root/.deeppavlov/downloads/dstc2\n",
            "2022-03-01 10:56:17.924 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/slotfill_dstc2.tar.gz to /root/.deeppavlov/slotfill_dstc2.tar.gz\n",
            "100%|██████████| 641k/641k [00:00<00:00, 1.29MB/s]\n",
            "2022-03-01 10:56:19.145 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/slotfill_dstc2.tar.gz archive into /root/.deeppavlov/models\n",
            "2022-03-01 10:56:19.858 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/gobot_dstc2_v9.tar.gz to /root/.deeppavlov/gobot_dstc2_v9.tar.gz\n",
            "100%|██████████| 966k/966k [00:00<00:00, 1.79MB/s]\n",
            "2022-03-01 10:56:21.275 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/gobot_dstc2_v9.tar.gz archive into /root/.deeppavlov/models\n",
            "2022-03-01 10:56:22.58 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/datasets/dstc2_v3.tar.gz to /root/.deeppavlov/downloads/dstc2_v3.tar.gz\n",
            "100%|██████████| 506k/506k [00:00<00:00, 1.09MB/s]\n",
            "2022-03-01 10:56:23.209 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/downloads/dstc2_v3.tar.gz archive into /root/.deeppavlov/downloads/dstc2_v3\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "2022-03-01 10:56:24.862 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/gobot_dstc2/word.dict]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ac6d71328579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbot1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgobot_dstc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbot1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi, i want restaurant in the cheap pricerange'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, download, serialized)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcomponent_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_serialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/core/common/params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, serialized, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/core/common/registry.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model {} is not registered.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/core/common/registry.py\u001b[0m in \u001b[0;36mcls_from_str\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     40\u001b[0m                           .format(name))\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/models/go_bot/go_bot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlg_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLGManagerInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlu_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLUManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolicyNetworkParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_prediction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicyPrediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturized_tracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeaturizedTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/models/go_bot/policy/policy_network.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpHint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_graph_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmicrofrontend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio_microfrontend_op\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_graphdef\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_convert_graphdef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/experimental/microfrontend/python/ops/audio_microfrontend_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m _audio_microfrontend_op = load_library.load_op_library(\n\u001b[0;32m---> 26\u001b[0;31m     resource_loader.get_path_to_datafile(\"_audio_microfrontend_op.so\"))\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/load_library.py\u001b[0m in \u001b[0;36mload_op_library\u001b[0;34m(library_filename)\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;31m# pylint: disable=exec-used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m   \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m   \u001b[0;31m# Allow this to be recognized by AutoGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_IS_TENSORFLOW_PLUGIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD670nqlWZOM"
      },
      "source": [
        "Поддробные туториалы:\n",
        "\n",
        "Simple: https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/gobot_tutorial.ipynb\n",
        "\n",
        "Extended: https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/gobot_extended_tutorial.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2x9-j4oz08p"
      },
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Статьи\n",
        "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling, 2018 [[pdf]](http://aclweb.org/anthology/N18-2050)\n",
        "\n",
        "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction, 2018 [[pdf]](http://aclweb.org/anthology/N18-2118) \n",
        "\n",
        "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings, 2018 [[pdf]](https://arxiv.org/pdf/1805.08237.pdf)\n",
        "\n",
        "BERT for Joint Intent Classification and Slot Filling\n",
        " [[pdf]](https://arxiv.org/pdf/1902.10909.pdf)\n",
        "\n",
        "## Блоги\n",
        "[Как устроена Алиса](https://habr.com/company/yandex/blog/349372/)  "
      ]
    }
  ]
}